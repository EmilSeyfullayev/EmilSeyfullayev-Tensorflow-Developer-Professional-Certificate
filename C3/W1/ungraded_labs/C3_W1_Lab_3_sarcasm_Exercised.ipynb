{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilSeyfullayev/EmilSeyfullayev-Tensorflow-Developer-Professional-Certificate/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_3_sarcasm_Exercised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBhiF0ehDgr0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_3_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNGfEo2u-r7"
      },
      "source": [
        "# Ungraded Lab: Tokenizing the Sarcasm Dataset\n",
        "\n",
        "In this lab, you will be applying what you've learned in the past two exercises to preprocess the [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home). This contains news headlines which are labeled as sarcastic or not. You will revisit this dataset in later labs so it is good to be acquainted with it now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twhyfjg0xTkg"
      },
      "source": [
        "## Download and inspect the dataset\n",
        "\n",
        "First, you will fetch the dataset and preview some of its elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "33W129a7xgoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62b8416-9520-4446-9e8f-de277cd6a8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-13 08:38:42--  https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.125.128, 142.250.157.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘sarcasm.json’\n",
            "\n",
            "\rsarcasm.json          0%[                    ]       0  --.-KB/s               \rsarcasm.json        100%[===================>]   5.38M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-13 08:38:42 (195 MB/s) - ‘sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJHdzh9FyWa2"
      },
      "source": [
        "The dataset is saved as a [JSON](https://www.json.org/json-en.html) file and you can use Python's [`json`](https://docs.python.org/3/library/json.html) module to load it into your workspace. The cell below unpacks the JSON file into a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OkaBMeNDwMel"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"./sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2aSBvJVzRNV"
      },
      "source": [
        "You can inspect a few of the elements in the list. You will notice that each element consists of a dictionary with a URL link, the actual headline, and a label named `is_sarcastic`. Printed below are two elements with contrasting labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RiiFcWU2xnMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8cdcae-8899-45d7-fb6d-9cb4ac68d62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n",
            "{'article_link': 'https://www.theonion.com/pediatricians-announce-2011-newborns-are-ugliest-babies-1819572977', 'headline': 'pediatricians announce 2011 newborns are ugliest babies in 30 years', 'is_sarcastic': 1}\n"
          ]
        }
      ],
      "source": [
        "# Non-sarcastic headline\n",
        "print(datastore[0])\n",
        "\n",
        "# Sarcastic headline\n",
        "print(datastore[20000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPuH0bBiz8LJ"
      },
      "source": [
        "With that, you can collect all urls, headlines, and labels for easier processing when using the tokenizer. For this lab, you will only need the headlines but we included the code to collect the URLs and labels as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9pxLUQJCxkNB"
      },
      "outputs": [],
      "source": [
        "# Initialize lists\n",
        "sentences = [] \n",
        "labels = []\n",
        "urls = []\n",
        "\n",
        "# Append elements in the dictionaries into each list\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBHSXJ5V0qqK"
      },
      "source": [
        "## Preprocessing the headlines\n",
        "\n",
        "You can convert the `sentences` list above into padded sequences by using the same methods you've been using in the past exercises. The cell below generates the `word_index` dictionary and generates the list of padded sequences for each of the 26,709 headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OSTw3uJuvmY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Print the length of the word index\n",
        "word_index = tokenizer.word_index\n",
        "print(f'number of words in word_index: {len(word_index)}')\n",
        "\n",
        "# Print the word index\n",
        "print(f'word_index: {word_index}')\n",
        "print()\n",
        "\n",
        "# Generate and pad the sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Print a sample headline\n",
        "index = 2\n",
        "print(f'sample headline: {sentences[index]}')\n",
        "print(f'padded sequence: {padded[index]}')\n",
        "print()\n",
        "\n",
        "# Print dimensions of padded sequences\n",
        "print(f'shape of padded sequences: {padded.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wyLF5T036W8"
      },
      "source": [
        "This concludes the short demo on using text data preprocessing APIs on a relatively large dataset. Next week, you will start building models that can be trained on these output sequences. See you there!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=30000, oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "MxfjWMAcEDIH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "oetsS6DKEjz6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sentences).shape"
      ],
      "metadata": {
        "id": "4PNaO4HoEbub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8820e6-3038-4696-bf35-985c163f83b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(sentences)"
      ],
      "metadata": {
        "id": "pIVIl61UETHd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "ueT1geiYEoBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca6744d-8497-4dc5-97aa-f465e191e840"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "FFH84bolE4Gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9c9ac1-9213-472e-938a-9626915475c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29657"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index['with']"
      ],
      "metadata": {
        "id": "6QH7hnMAFthQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb92c91e-0559-49f3-fd45-e9e5b5604106"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_index = tokenizer.word_index not necesserily to run"
      ],
      "metadata": {
        "id": "P5JpLNY9F3cy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "id": "00lUiosXGFmg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences).shape"
      ],
      "metadata": {
        "id": "PhDka5YUGmZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faecccf2-8d18-4453-e7ad-e8f335b3dc83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:10]"
      ],
      "metadata": {
        "id": "YESR8Fg3GsjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01cca45-3cf0-47b9-eadf-b984b1ab91f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[308, 15115, 679, 3337, 2298, 48, 382, 2576, 15116, 6, 2577, 8434],\n",
              " [4, 8435, 3338, 2746, 22, 2, 166, 8436, 416, 3112, 6, 258, 9, 1002],\n",
              " [145, 838, 2, 907, 1749, 2093, 582, 4719, 221, 143, 39, 46, 2, 10736],\n",
              " [1485, 36, 224, 400, 2, 1832, 29, 319, 22, 10, 2924, 1393, 6969, 968],\n",
              " [767, 719, 4720, 908, 10737, 623, 594, 5, 4, 95, 1309, 92],\n",
              " [10738, 4, 365, 73],\n",
              " [4, 6970, 351, 6, 461, 4274, 2195, 1486],\n",
              " [19, 479, 39, 1168, 31, 155, 2, 99, 83, 18, 158, 6, 32, 352],\n",
              " [249, 3623, 6971, 555, 5274, 1995, 141],\n",
              " [2094, 326, 347, 401, 60, 15117, 6, 4, 3896]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences)[:10]"
      ],
      "metadata": {
        "id": "4rUFrHDBG2N4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c35d3ad-6cfa-40e0-8c78-fba8e28bd2f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([308, 15115, 679, 3337, 2298, 48, 382, 2576, 15116, 6, 2577, 8434]),\n",
              "       list([4, 8435, 3338, 2746, 22, 2, 166, 8436, 416, 3112, 6, 258, 9, 1002]),\n",
              "       list([145, 838, 2, 907, 1749, 2093, 582, 4719, 221, 143, 39, 46, 2, 10736]),\n",
              "       list([1485, 36, 224, 400, 2, 1832, 29, 319, 22, 10, 2924, 1393, 6969, 968]),\n",
              "       list([767, 719, 4720, 908, 10737, 623, 594, 5, 4, 95, 1309, 92]),\n",
              "       list([10738, 4, 365, 73]),\n",
              "       list([4, 6970, 351, 6, 461, 4274, 2195, 1486]),\n",
              "       list([19, 479, 39, 1168, 31, 155, 2, 99, 83, 18, 158, 6, 32, 352]),\n",
              "       list([249, 3623, 6971, 555, 5274, 1995, 141]),\n",
              "       list([2094, 326, 347, 401, 60, 15117, 6, 4, 3896])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_one = [[1, 2, 3], [1, 2, 3]]\n",
        "print(np.array(list_one))\n",
        "np.array(list_one).shape"
      ],
      "metadata": {
        "id": "9FrW9FkdHAOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bc3f1f-6fd2-4c29-f70d-3157887056d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [1 2 3]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_two = [[1, 2, 3], [1, 2]]\n",
        "print(np.array(list_two))\n",
        "np.array(list_two).shape"
      ],
      "metadata": {
        "id": "IUKgcxCqHX2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4c6f07-a002-4a5b-86f3-ff14efc24981"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([1, 2, 3]) list([1, 2])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences, padding='post')"
      ],
      "metadata": {
        "id": "l9-W3CdFHcp7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded.shape"
      ],
      "metadata": {
        "id": "-7gDmnBtJSPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b593fc-5ff6-45ba-ebce-decd8bbfc518"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded[:10]"
      ],
      "metadata": {
        "id": "2zbfljPfJTZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e8c9c5-7b1b-4406-d007-9ef871f41756"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  308, 15115,   679,  3337,  2298,    48,   382,  2576, 15116,\n",
              "            6,  2577,  8434,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [    4,  8435,  3338,  2746,    22,     2,   166,  8436,   416,\n",
              "         3112,     6,   258,     9,  1002,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [  145,   838,     2,   907,  1749,  2093,   582,  4719,   221,\n",
              "          143,    39,    46,     2, 10736,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [ 1485,    36,   224,   400,     2,  1832,    29,   319,    22,\n",
              "           10,  2924,  1393,  6969,   968,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [  767,   719,  4720,   908, 10737,   623,   594,     5,     4,\n",
              "           95,  1309,    92,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [10738,     4,   365,    73,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [    4,  6970,   351,     6,   461,  4274,  2195,  1486,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [   19,   479,    39,  1168,    31,   155,     2,    99,    83,\n",
              "           18,   158,     6,    32,   352,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [  249,  3623,  6971,   555,  5274,  1995,   141,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0],\n",
              "       [ 2094,   326,   347,   401,    60, 15117,     6,     4,  3896,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LmmqwOubJXy6"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of C3_W1_Lab_3_sarcasm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}